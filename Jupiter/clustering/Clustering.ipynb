{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gmr = nx.read_graphml(\"data/Marta_Rovira-link-list_out.graphml\")\n",
    "Gv8 = nx.read_graphml(\"data/Vaga8Nov-link-list_out.graphml\")\n",
    "Gnb = nx.read_graphml(\"data/nochebuena-link-list_out.graphml\")\n",
    "Gbml = nx.read_graphml(\"data/BLM-link-list_out.graphml\")\n",
    "Gcov = nx.read_graphml(\"data/COVID19-link-list_out.graphml\")\n",
    "Gpri = nx.read_graphml(\"data/primavera-link-list_out.graphml\")\n",
    "Gqua = nx.read_graphml(\"data/quarantine-link-list_out.graphml\")\n",
    "Gsj = nx.read_graphml(\"data/santjordi-link-list_out.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nx.info(Gpri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Gv8.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(Gpri.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bcb21e72c884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGmr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'community'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_vaga\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGv8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'community'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_nochebuena\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'community'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_blm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGbml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'community'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata_covid19\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGcov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'community'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data_mr = pd.DataFrame.from_dict(dict(Gmr.nodes), orient='index').drop(columns=['community'])\n",
    "data_vaga = pd.DataFrame.from_dict(dict(Gv8.nodes), orient='index').drop(columns=['community'])\n",
    "data_nochebuena = pd.DataFrame.from_dict(dict(Gnb.nodes), orient='index').drop(columns=['community'])\n",
    "data_blm = pd.DataFrame.from_dict(dict(Gbml.nodes), orient='index').drop(columns=['community'])\n",
    "data_covid19 = pd.DataFrame.from_dict(dict(Gcov.nodes), orient='index').drop(columns=['community'])\n",
    "data_primavera = pd.DataFrame.from_dict(dict(Gpri.nodes), orient='index').drop(columns=['community'])\n",
    "data_quarantine = pd.DataFrame.from_dict(dict(Gqua.nodes), orient='index').drop(columns=['community'])\n",
    "data_santjordi = pd.DataFrame.from_dict(dict(Gsj.nodes), orient='index').drop(columns=['community'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_primavera.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_primavera.dispersion_index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sources = {\n",
    "    \"Marta_Rovira\": data_mr,\n",
    "    \"Nochebuena\": data_nochebuena,\n",
    "    \"Vaga8Nov\": data_vaga, \n",
    "    \"BLM\": data_blm,\n",
    "    \"COVID19\": data_covid19,\n",
    "    \"primavera\": data_primavera,\n",
    "    \"quarantine\": data_quarantine,\n",
    "    \"sant_jordi\": data_santjordi\n",
    "}\n",
    "dict_sources.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(train_df, hyperparameters, name, isVisualized):\n",
    "    print('Dataset: ', name)\n",
    "    stats_df = pd.DataFrame(columns=['params', 'num_clusters', 'noise_pts'])\n",
    "    \n",
    "    # PCA\n",
    "    data_subset = train_df.values\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_result = pca.fit_transform(data_subset)\n",
    "    train_df['pca-one'] = pca_result[:,0]\n",
    "    train_df['pca-two'] = pca_result[:,1] \n",
    "    train_df['pca-three'] = pca_result[:,2]    \n",
    "    print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "        \n",
    "        \n",
    "    for eps, minPts in hyperparameters:\n",
    "        start = time.time()\n",
    "        print('EPS:', eps, ' minPts:', minPts)\n",
    "        clustering = DBSCAN(\n",
    "            eps=eps, \n",
    "            min_samples=minPts,\n",
    "            n_jobs=-1\n",
    "        ).fit(train_df)\n",
    "        labels_s = pd.Series(clustering.labels_)\n",
    "        n_clusters = len(set(labels_s)) - (1 if -1 in labels_s else 0)\n",
    "        n_noise = list(labels_s).count(-1)\n",
    "        DB = metrics.davies_bouldin_score(train_df, labels_s)\n",
    "        silhouette = metrics.silhouette_score(train_df, labels_s)\n",
    "        stats_df = stats_df.append(\n",
    "            {\n",
    "                'params': str(eps) + '/' + str(minPts),\n",
    "                'num_clusters': n_clusters,\n",
    "                'noise_pts': n_noise,\n",
    "                'DB': DB,\n",
    "                'silhouette': silhouette\n",
    "            },\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        print('Estimated number of clusters: %d' % n_clusters)\n",
    "        print('Estimated number of noise points: %d' % n_noise)\n",
    "        if n_clusters > 1 and n_clusters < 10000:\n",
    "            print('DB: ', DB)\n",
    "            print('Silhouette: ', silhouette)\n",
    "        elif n_clusters >= 5000:\n",
    "            print('TOO MANY CLUSTERS!')\n",
    "            continue\n",
    "        else:\n",
    "            print('NO CLUSTERS FORMED!')\n",
    "            continue\n",
    "        print('Time: ', time.time() - start)\n",
    "        print('-------------------------------------------------------------')\n",
    "        \n",
    "        if isVisualized == True:\n",
    "            train_df['label'] = labels_s\n",
    "            visualization(train_df, (eps, minPts), set(labels_s))\n",
    "\n",
    "    if isVisualized != True: \n",
    "        plt.figure(figsize=(16,10))\n",
    "        sns.barplot(x=\"params\", y=\"num_clusters\", data=stats_df).set_title(name + \" dataset\")\n",
    "        plt.figure(figsize=(16,10))\n",
    "        sns.barplot(x=\"params\", y=\"noise_pts\", data=stats_df).set_title(name + \" dataset\")\n",
    "        plt.figure(figsize=(16,10))\n",
    "        sns.barplot(x=\"params\", y=\"DB\", data=stats_df).set_title(name + \" dataset\")\n",
    "        plt.figure(figsize=(16,10))\n",
    "        sns.barplot(x=\"params\", y=\"silhouette\", data=stats_df).set_title(name + \" dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(train_df, params, labels):\n",
    "    (eps, minPts) = params\n",
    "    \n",
    "    # TSNE Training\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(train_df.values)\n",
    "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "    # VISUALIZATION\n",
    "    train_df['tsne-first'] = tsne_results[:,0]\n",
    "    train_df['tsne-second'] = tsne_results[:,1]\n",
    "    plt.figure(figsize=(16,10))\n",
    "    g = sns.scatterplot(\n",
    "        x=\"tsne-first\", \n",
    "        y='tsne-second',\n",
    "        hue=\"label\",\n",
    "        palette=sns.color_palette(\"hls\", len(labels)),\n",
    "        data=train_df,\n",
    "        legend=False,\n",
    "        alpha=0.3\n",
    "    ).set_title('Params: ' + str(eps) + ', ' + str(minPts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DB index score**: the lower the better<br>\n",
    "**Silhouette coefficient**: (-1, 1) the higher the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50000\n",
    "\n",
    "params = []\n",
    "for eps in [0.1, 0.3, 0.5, 1, 1.5, 2]:\n",
    "    for pts in [2, 3, 5, 8]:\n",
    "        params.append((eps, pts))\n",
    "\n",
    "for name, df in dict_sources.items():\n",
    "    train = df.iloc[:N,:].copy().reset_index(drop=True)\n",
    "    cluster(train, params, name, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {\n",
    "    \"Marta_Rovira\": [(2, 5)],\n",
    "    \"Nochebuena\": [(2, 3)],\n",
    "    \"Vaga8Nov\": [(2, 5)], \n",
    "    \"BLM\": [(2, 2)],\n",
    "    \"COVID19\": [(2, 2)],\n",
    "    \"primavera\": [(2, 2)],\n",
    "    \"quarantine\": [(2, 2)],\n",
    "    \"sant_jordi\": [(2, 2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dict_sources.items():\n",
    "    train = df.copy().reset_index(drop=True)\n",
    "    cluster(train, dict_params[name], name, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
